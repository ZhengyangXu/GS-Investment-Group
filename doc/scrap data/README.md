﻿This part is mainly focus on get data from open source. I built two spiders to scrape New York City real estate sales in the past 9 months (from May 2018 till Feb 2019) and one spider to scrape New York City Upper West Side real estate data.The first one, nyz_spider was built to parse individual listing pages for information. After scraping for a brief period of time, the spider was rejected by the trulia.com website.The second spider: nyz_lite_spider that instead parses the result pages only for information. This was more successful, producing over 13,000 observations. However, this method meant abandoning the Property Type and Age features.The third spider, I used API of ATTOM (https://www.attomdata.com) to scrape data. I just used the detail API and there are more APIS to explore. Totally got 300 observations, which is weird. I think it may relate to the API's own limitation.And the scraped data are sorted in the data file.